{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2d2b8fd",
   "metadata": {},
   "source": [
    "This notebook contains code made for deriving snow depths from the ICESat-2 ATL03 Dataset. \n",
    "\n",
    "Workflow:\n",
    "\n",
    "- Estimate surface height from ICESat-2 ATL03 data\n",
    "\n",
    "Required input data: \n",
    "\n",
    "- ATL03 Trackline\n",
    "- Digital Elevation Model\n",
    "- Masks (Optional) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e6a623",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Package Import ##\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py  \n",
    "import os\n",
    "import fiona\n",
    "import geopandas\n",
    "import time\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import scipy\n",
    "from astropy.time import Time\n",
    "print(geopandas.__version__) # Version 0.8.1\n",
    "\n",
    "import rasterio\n",
    "import geopandas as gpd\n",
    "import pyproj\n",
    "import rasterio\n",
    "import glob\n",
    "from numpy import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187fbc96",
   "metadata": {},
   "source": [
    "getATL03 function loads in raw ICESat-2 ATL03 data in .h5 format and outputs are geodataframe where each row represents a photon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750d3aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getATL03(f,beam):\n",
    "    # height of each received photon, relative to the WGS-84 ellipsoid (with some, not all corrections applied, see background info above)\n",
    "    heights=f[beam]['heights']['h_ph'][:]\n",
    "    # latitude (decimal degrees) of each received photon\n",
    "    lats=f[beam]['heights']['lat_ph'][:]\n",
    "    # longitude (decimal degrees) of each received photon\n",
    "    lons=f[beam]['heights']['lon_ph'][:]\n",
    "    # seconds from ATLAS Standard Data Product Epoch. use the epoch parameter to convert to gps time\n",
    "    dt=f[beam]['heights']['delta_time'][:]\n",
    "    # confidence level associated with each photon event\n",
    "    # -2: TEP\n",
    "    # -1: Events not associated with a specific surface type\n",
    "    #  0: noise\n",
    "    #  1: buffer but algorithm classifies as background\n",
    "    #  2: low\n",
    "    #  3: medium\n",
    "    #  4: high\n",
    "    # Surface types for signal classification confidence\n",
    "    # 0=Land; 1=Ocean; 2=SeaIce; 3=LandIce; 4=InlandWater    \n",
    "    conf=f[beam]['heights']['signal_conf_ph'][:,0] #choose column 2 for confidence of sea ice photons\n",
    "    # number of ATL03 20m segments\n",
    "    n_seg, = f[beam]['geolocation']['segment_id'].shape\n",
    "    #GEOID\n",
    "    geoid = f[beam]['geophys_corr']['geoid'][:]\n",
    "    # first photon in the segment (convert to 0-based indexing)\n",
    "    Segment_Index_begin = f[beam]['geolocation']['ph_index_beg'][:] - 1\n",
    "    # number of photon events in the segment\n",
    "    Segment_PE_count = f[beam]['geolocation']['segment_ph_cnt'][:]\n",
    "    # along-track distance for each ATL03 segment\n",
    "    Segment_Distance = f[beam]['geolocation']['segment_dist_x'][:]\n",
    "    # along-track distance (x) for photon events\n",
    "    x_atc = np.copy(f[beam]['heights']['dist_ph_along'][:])\n",
    "    # cross-track distance (y) for photon events\n",
    "    y_atc = np.copy(f[beam]['heights']['dist_ph_across'][:])\n",
    "\n",
    "    for j in range(n_seg):\n",
    "        # index for 20m segment j\n",
    "        idx = Segment_Index_begin[j]\n",
    "        # number of photons in 20m segment\n",
    "        cnt = Segment_PE_count[j]\n",
    "        # add segment distance to along-track coordinates\n",
    "        x_atc[idx:idx+cnt] += Segment_Distance[j]\n",
    "        #geoid\n",
    "        #geoid[idx:idx+cnt] += geoid[j]\n",
    "\n",
    "\n",
    "    df03=pd.DataFrame({'lats':lats,'lons':lons,'x':x_atc,'y':y_atc,'heights':heights,'dt':dt,'conf':conf})\n",
    "    return df03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36411cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ATL03(path, date, beam):\n",
    "    ATL03_path = path + date + '/ATL03/'\n",
    "    for file in os.listdir(ATL03_path):\n",
    "        if file.endswith('.h5'):\n",
    "            fname = file\n",
    "    f = h5py.File(ATL03_path+fname,'r')\n",
    "    geoid = f[beam]['geophys_corr']['geoid'][:]\n",
    "    #10m res\n",
    "    geoid = list(np.repeat(geoid, 2))\n",
    "    #100m res\n",
    "    #geoid = geoid[::5]\n",
    "    #geoid = geoid.tolist()\n",
    "    df = getATL03(f,beam)\n",
    "    df = df[df['conf'] > 2]\n",
    "    df['AT_dist']=df.x-df.x.values[0]\n",
    "    return df, geoid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98570b3d",
   "metadata": {},
   "source": [
    "Surf function estimates the surface height by grouping photons height values along-track, using kernel density to estimate surface height, and substracting the geoid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c151b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def surface(dfATL03, window_width, geoid, resolution):\n",
    "    \n",
    "    '''\n",
    "    Input:\n",
    "        dfATL03:\n",
    "        window_width:\n",
    "        geoid:\n",
    "        resolution:\n",
    "    Output:\n",
    "        df:\n",
    "    '''\n",
    "    \n",
    "    # Track function time\n",
    "    startTime = time.time()\n",
    "    \n",
    "    # Along track distance values, min and max to determine boundries for moving window.\n",
    "    AT_dist_values = dfATL03['AT_dist'].values\n",
    "    AT_dist_totmax = AT_dist_values.max()\n",
    "    AT_dist_totmin = AT_dist_values.min()\n",
    "    \n",
    "    \n",
    "    # Photon height level array\n",
    "    heights_values = dfATL03['heights'].values\n",
    "    \n",
    "    # Confidence level array\n",
    "    conf_values = dfATL03['conf'].values\n",
    "    \n",
    "    # Coordinates array\n",
    "    lats = dfATL03['lats'].values\n",
    "    lons = dfATL03['lons'].values\n",
    "    \n",
    "    # Moving Window - List of values starting from Along track distance min to max,\n",
    "    # at a resulution of either 10 or 100 meters\n",
    "    windows = np.arange(AT_dist_totmin, AT_dist_totmax, resolution).tolist()\n",
    "\n",
    "\n",
    "    # Empty arrays to store new along track segmented values instead of per-photon.\n",
    "    \n",
    "    #Along track window center array\n",
    "    AT_window_center_arr = np.empty(len(windows))\n",
    "    \n",
    "    # Surface array\n",
    "    surface_arr = np.empty(len(windows))  \n",
    "    \n",
    "    # Latitude\n",
    "    lat_arr = np.empty(len(windows))  \n",
    "    \n",
    "    # Longitude\n",
    "    lon_arr = np.empty(len(windows))  \n",
    "    \n",
    "    # track number of iterations\n",
    "    i = 0\n",
    "    \n",
    "    # Iterate through every photon(row), store then into segments at x resolution and calculate surface\n",
    "    # height based on photon height density.\n",
    "    for window_center in windows:    \n",
    " \n",
    "        # Get minimum window boundries\n",
    "        min_dist = window_center - window_width\n",
    "        min_dist_array = np.where(AT_dist_values > min_dist)\n",
    "        min_dist_row = min_dist_array[0][0]\n",
    "\n",
    "        # Get maximum window boundries\n",
    "        max_dist = window_center + window_width\n",
    "        if max_dist < AT_dist_values[-1]:\n",
    "            max_dist_array = np.where(AT_dist_values > max_dist)\n",
    "            max_dist_row = max_dist_array[0][0]\n",
    "        else:\n",
    "            max_dist = AT_dist_values[-1] \n",
    "\n",
    "        # Get window center lat & long (for plotting)     \n",
    "        idx = (np.abs(AT_dist_values - window_center)).argmin()\n",
    "        lat = lats[idx]\n",
    "        lon = lons[idx]\n",
    "\n",
    "        # Select photons AT_dist & heights within window boundries\n",
    "        window_heights = heights_values[min_dist_row:max_dist_row]\n",
    "\n",
    "        # Only Search for surface if there is enough Photons in window\n",
    "        if len(window_heights) > 10: #Normally 5\n",
    "            \n",
    "            binwidth = 0.2\n",
    "            \n",
    "            # Kernel Density Function\n",
    "            kde = KernelDensity(kernel='gaussian', bandwidth=binwidth).fit(window_heights[:, None])\n",
    "            \n",
    "            # Kernel density scores\n",
    "            kde_scores = kde.score_samples(window_heights[:, None])\n",
    "            \n",
    "            # Select bin with heighest density\n",
    "            maxdensityI = np.max(kde_scores)\n",
    "            \n",
    "            # Extract height from heighest density bin\n",
    "            max_index = np.where(kde_scores == maxdensityI)\n",
    "            maxdensityH =  window_heights[max_index]\n",
    "            \n",
    "            # Define surface-height as height with highest photon density  \n",
    "            Surface_Estimation = maxdensityH[0]\n",
    "            \n",
    "            # Add center of window as new along track distance in element i\n",
    "            AT_window_center_arr[i] = window_center \n",
    "            \n",
    "            # Add estimated surface\n",
    "            surface_arr[i] = Surface_Estimation\n",
    "            \n",
    "            # Add Latitude\n",
    "            lat_arr[i] = lat\n",
    "            \n",
    "            # Add longitude\n",
    "            lon_arr[i] = lon\n",
    "            \n",
    "            i+=1\n",
    "\n",
    "        else:\n",
    "            \n",
    "            # If there is not enough photons in window, set surface to 0. \n",
    "            AT_window_center_arr[i] = window_center\n",
    "            surface_arr[i] = 0\n",
    "            lat_arr[i] = lat\n",
    "            lon_arr[i] = lon     \n",
    "            i+=1\n",
    "        \n",
    "        # Calculate and print progress\n",
    "        wincen = int(window_center)\n",
    "        if wincen % 25000 < 1:  \n",
    "            m = int(i / len(windows) * 100)\n",
    "            print(m, \"% Done\")\n",
    "    \n",
    "    # Create new segmented dataframe at x resolution with surface heights, along track distance and coordinates. \n",
    "    df_surface = pd.DataFrame(AT_window_center_arr, columns = ['AT_dist']) \n",
    "    df_surface[\"Surface\"] = surface_arr\n",
    "    df_surface[\"lats\"] = lat_arr\n",
    "    df_surface[\"lons\"] = lon_arr\n",
    "\n",
    "    # Add geoid to correct surface heights when comparing heights to Digital Elevation Models with a different coordinate system.\n",
    "    if len(geoid) < len(df_surface.index):\n",
    "      while len(geoid) < len(df_surface.index):\n",
    "        geoid.append(geoid[-1])\n",
    "      df_surface[\"geoid\"] = geoid\n",
    "    elif len(geoid) > len(df_surface.index):\n",
    "       df_surface[\"geoid\"] = geoid[:len(df_surface.index)]   \n",
    "    else:\n",
    "      df_surface[\"geoid\"] = geoid\n",
    "\n",
    "    # Calculate corrected surface height\n",
    "    df_surface['Surface_corr'] = (df_surface['Surface'] - df_surface['geoid']).round(2)\n",
    "    \n",
    "    # Script Runtime\n",
    "    runTime =  int(time.time() - startTime)\n",
    "    runTimeMin = runTime/60\n",
    "    runTimeSec = runTime%60\n",
    "    print(\"\\nScript Runtime: %i minutes and %i seconds\" % (runTimeMin,runTimeSec))\n",
    "    \n",
    "    return df_surface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fba3bac",
   "metadata": {},
   "source": [
    "df_to_shp converts every estimated surface height to a point in a shapefile. These points are used further on for snow depth estimations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b871dab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def snow_depth(df_surface, beam, DEM):\n",
    "        \n",
    "    '''\n",
    "    Input:\n",
    "        df_surface:\n",
    "        beam:\n",
    "        DEM:\n",
    "    Output:\n",
    "        pts:\n",
    "    '''\n",
    "    #Create Geodataframe from dataframe\n",
    "    df_sd = df_surface\n",
    "    df_sd = geopandas.GeoDataFrame(\n",
    "    df_sd, geometry=geopandas.points_from_xy(df_sd.lons, df_sd.lats))\n",
    "\n",
    "    #Set coordinate system\n",
    "    df_sd = df_sd.set_crs('EPSG:4326')\n",
    "\n",
    "    #Convert coordinate system\n",
    "    df_sd = df_sd.to_crs(25833)       \n",
    "\n",
    "    #Sample DEM to Icesat-2 points\n",
    "    coords = [(x,y) for x, y in zip(df_sd.geometry.x, df_sd.geometry.y)]\n",
    "    src = rasterio.open(DEM)\n",
    "    df_sd['DEM'] = [x[0] for x in src.sample(coords)]\n",
    "\n",
    "    #Norwegian DEM\n",
    "    df_sd['SD'] = round(df_sd.Surface_corr - df_sd.DEM,2)\n",
    "\n",
    "    #Arctic DEM - Using same coordinate system as IS2, so no need to correct.\n",
    "    #pts['SD'] = round(pts.Surface - pts.DEM,2)\n",
    "\n",
    "    print('Training Points: ',df_sd.shape[0])\n",
    "\n",
    "    return df_sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6743808e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Mask(df_sd, path, date):\n",
    "    \n",
    "    df_sd_masked = df_sd\n",
    "    coords = [(x,y) for x, y in zip(df_sd_masked.geometry.x, df_sd_masked.geometry.y)]\n",
    "\n",
    "    SLOPE = path[:-7] + '/Masks/Slope.tif'\n",
    "    Binary_Mask = path + date + '/Masks/Binary_Mask.tif'\n",
    "    listofmasks=[SLOPE, Binary_Mask]\n",
    "    listofmasksname=['SLOPE', 'Binary_Mask']\n",
    "    \n",
    "    i = 0\n",
    "    for mask in listofmasks:\n",
    "        src = rasterio.open(mask)\n",
    "        df_sd_masked[listofmasksname[i]] = [x[0] for x in src.sample(coords)]\n",
    "        i=i+1\n",
    "        \n",
    "    \n",
    "    print('Training Points: ',df_sd_masked.shape[0])\n",
    "\n",
    "    #Mask out points on slope over 8 degrees\n",
    "    df_sd_masked = df_sd_masked[df_sd_masked['SLOPE'] < 8]\n",
    "    print('Slope: ',df_sd_masked.shape[0])\n",
    "    \n",
    "    df_sd_masked = df_sd_masked[df_sd_masked['Binary_Mask'] > 0]\n",
    "    print('Binary Mask: ',df_sd_masked.shape[0])\n",
    "\n",
    "    \n",
    "    print('Outliers: ',df_sd_masked.shape[0])\n",
    "\n",
    "    print('Training data points left after masking: ', df_sd_masked.shape[0])\n",
    "    \n",
    "    return df_sd_masked\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c501c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Rescale(df_sd_masked, window_width, rescale_resolution, beam, path, date):\n",
    "\n",
    "    # Along track distance values, min and max to determine boundries for moving window.\n",
    "    AT_dist_values = df_sd_masked['AT_dist'].values\n",
    "    AT_dist_totmax = AT_dist_values.max()\n",
    "    AT_dist_totmin = AT_dist_values.min()\n",
    "\n",
    "    # Coordinates\n",
    "    df_sd_masked['lons'] = df_sd_masked.geometry.x\n",
    "    df_sd_masked['lats']= df_sd_masked.geometry.y\n",
    "\n",
    "    # Coordinates array\n",
    "    lats = df_sd_masked['lats'].values\n",
    "    lons = df_sd_masked['lons'].values\n",
    "\n",
    "    # Snow depth array\n",
    "    SD_values = df_sd_masked['SD'].values\n",
    "\n",
    "\n",
    "    # Moving Window - List of values starting from Along track distance min to max,\n",
    "    # at a resulution of x meters.\n",
    "    windows = np.arange(AT_dist_totmin, AT_dist_totmax, rescale_resolution).tolist()\n",
    "\n",
    "    #Along track window center array\n",
    "    AT_window_center_arr = np.empty(len(windows))\n",
    "    \n",
    "    # Snow depth array\n",
    "    snow_depth_arr = np.empty(len(windows))  \n",
    "    \n",
    "    # Latitude\n",
    "    lat_arr = np.empty(len(windows))  \n",
    "    \n",
    "    # Longitude\n",
    "    lon_arr = np.empty(len(windows)) \n",
    "\n",
    "    # track number of iterations\n",
    "    i = 0\n",
    "    \n",
    "    # Iterate through every photon(row), store then into segments at x resolution and calculate surface\n",
    "    # height based on photon height density.\n",
    "    for window_center in windows:    \n",
    "\n",
    "        # Get minimum window boundries\n",
    "        min_dist = window_center - window_width\n",
    "        min_dist_array = np.where(AT_dist_values > min_dist)\n",
    "        min_dist_row = min_dist_array[0][0]\n",
    "\n",
    "        # Get maximum window boundries\n",
    "        max_dist = window_center + window_width\n",
    "        if max_dist < AT_dist_values[-1]:\n",
    "            max_dist_array = np.where(AT_dist_values > max_dist)\n",
    "            max_dist_row = max_dist_array[0][0]\n",
    "        else:\n",
    "            max_dist = AT_dist_values[-1] \n",
    "\n",
    "        # Get window center lat & long (for plotting)     \n",
    "        idx = (np.abs(AT_dist_values - window_center)).argmin()\n",
    "        lat = lats[idx]\n",
    "        lon = lons[idx]\n",
    "        \n",
    "        # Select snow depths within window boundries\n",
    "        window_SD = SD_values[min_dist_row:max_dist_row]\n",
    "        \n",
    "        # Only Search for snow depths if there is enough Photons in window\n",
    "        if len(window_SD) > 5: #Normally 5\n",
    "                \n",
    "            #Define new snow depth as median of snow depth within larger rescaled window boundries\n",
    "            SD = np.median(window_SD)\n",
    "\n",
    "\n",
    "            # Add center of window as new along track distance in element i.\n",
    "            AT_window_center_arr[i] = window_center \n",
    "            \n",
    "            # Add estimated surface\n",
    "            snow_depth_arr[i] = SD\n",
    "            \n",
    "            # Add Latitude\n",
    "            lat_arr[i] = lat\n",
    "            \n",
    "            # Add longitude\n",
    "            lon_arr[i] = lon\n",
    "\n",
    "            i+=1\n",
    "\n",
    "        else:\n",
    "            \n",
    "            # If there is not enough photons in window, set surface to -999. \n",
    "            AT_window_center_arr[i] = window_center\n",
    "            snow_depth_arr[i] = -999\n",
    "            lat_arr[i] = lat\n",
    "            lon_arr[i] = lon\n",
    "            i+=1\n",
    "\n",
    "    df = gpd.GeoDataFrame(AT_window_center_arr, columns = ['AT_dist']) \n",
    "    df[\"SD\"] = snow_depth_arr\n",
    "    df[\"lats\"] = lat_arr\n",
    "    df[\"lons\"] = lon_arr\n",
    "\n",
    "    gdf = geopandas.GeoDataFrame(\n",
    "    df, geometry=geopandas.points_from_xy(df.lons, df.lats))\n",
    "    gdf = gdf.set_crs('EPSG:25833')\n",
    "    \n",
    "    #Filter bad observations\n",
    "    gdf = gdf[gdf['SD'] > 0]\n",
    "    gdf = gdf[gdf['SD'] < 10]\n",
    "    gdf = gdf[gdf['SD'] != -999]\n",
    "    if gdf.shape[0] > 0:\n",
    "        outdir = path + date + '/SnowDepth/' + beam + '.shp'\n",
    "        gdf.to_file(outdir)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bd46d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sample(path, date):\n",
    "    \n",
    "    i=0\n",
    "    SD_path = path + date + '/SnowDepth/'\n",
    "    for file in os.listdir(SD_path):\n",
    "        if file.endswith('shp'):\n",
    "\n",
    "            points = os.path.join(SD_path,file)\n",
    "            pts = gpd.read_file(points)\n",
    "            \n",
    "            if i != 0:\n",
    "                df_train = pd.concat([df_train,pts])\n",
    "            else: \n",
    "                df_train = pts\n",
    "                i=i+1  \n",
    "                \n",
    "    pts = df_train\n",
    "    coords = [(x,y) for x, y in zip(pts.geometry.x, pts.geometry.y)]\n",
    "\n",
    "    VH = path + date + '/S1/VH.tif'\n",
    "    Diff = path + date + '/S1/Diff.tif'\n",
    "    Ratio = path + date + '/S1/Ratio.tif'\n",
    "    Subtract = path + date + '/S1/Subtract.tif'\n",
    "    DEM = path[:-7] + '/DEM/mergedDEM10m_Hardanger.tif'\n",
    "    \n",
    "    outdir = path + date + '/Sampled/SD.shp'\n",
    "    \n",
    "    listofmasks=[VH, Diff, Ratio, Subtract, DEM]\n",
    "    listofmasksname=['VH', 'Diff', 'Ratio', 'Subtract','DEM']\n",
    "    i = 0\n",
    "    for mask in listofmasks:\n",
    "        src = rasterio.open(mask)\n",
    "        pts[listofmasksname[i]] = [x[0] for x in src.sample(coords)]\n",
    "        i=i+1\n",
    "    pts = pts.drop(columns=['lats', 'lons', 'AT_dist'])    \n",
    "    pts.to_file(outdir)\n",
    "    return pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6f5ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(params):\n",
    "\n",
    "    #datelist = ['20200204','20200206','20200306','20200314','20200406','20200410','20210203','20210205','20210207','20210209','20210211','20210304','20210308']\n",
    "    datelist = ['20210211','20210304']\n",
    "    for dates in datelist:\n",
    "        params['date'] = dates\n",
    "        print('started processing of date: ', dates)\n",
    "    \n",
    "    \n",
    "        #beamlist = ['gt1l','gt2l','gt3l','gt1r','gt2r','gt3r']\n",
    "        beamlist = ['gt1l','gt2l']\n",
    "\n",
    "    #    i=0\n",
    "        for ISbeam in beamlist:\n",
    "            params['beam'] = ISbeam\n",
    "            df, geoid = load_ATL03(params['path'],params['date'], params['beam'])\n",
    "            print('ATL03 Dataset loaded')\n",
    "\n",
    "            df_surf = surface(df, 10, geoid, 10)\n",
    "            df_surf = df_surf[df_surf['Surface'] != 0]\n",
    "            print('Surface Heights Estimated')\n",
    "\n",
    "            df_SD = snow_depth(df_surf, params['beam'], params['DEM'])\n",
    "            print('Snow Depths Estimated')\n",
    "\n",
    "            df_Mask = Mask(df_SD, params['path'], params['date'])\n",
    "            print('Masked out training points')\n",
    "\n",
    "            df_RE = Rescale(df_Mask, 3000, 1000, params['beam'], params['path'], params['date'])\n",
    "            print('Rescaled training points')\n",
    "            print('completed ICESat-2 Processing for beam: ',ISbeam)\n",
    "\n",
    "            pts = Sample(params['path'], params['date'])\n",
    "        \n",
    "    return df, df_surf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdba9340",
   "metadata": {},
   "outputs": [],
   "source": [
    "Load_params = {\n",
    "    'path' : 'C:/Users/Rasmu/Documents/Thesis/Hardangervidda/Dates/',\n",
    "    'date' : '20200306',\n",
    "    #'DEM': 'C:/Users/Rasmu/Documents/Thesis/DEM/ArcticDEM_10m.tif',\n",
    "    'DEM': 'C:/Users/Rasmu/Documents/Thesis/DEM/mergedDEM10m_Hardanger.tif'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d1a5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, df_surf = run(Load_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2242838",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
