{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2d2b8fd",
   "metadata": {},
   "source": [
    "This notebook contains code made for deriving snow depths from the ICESat-2 ATL03 Dataset. \n",
    "\n",
    "Workflow:\n",
    "\n",
    "- Estimate surface height from ICESat-2 ATL03 data\n",
    "\n",
    "Required input data: \n",
    "\n",
    "- ATL03 Trackline\n",
    "- Digital Elevation Model\n",
    "- Masks (Optional) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "65e6a623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8.1\n"
     ]
    }
   ],
   "source": [
    "## Package Import ##\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py  \n",
    "import os\n",
    "import fiona\n",
    "import geopandas\n",
    "import time\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import scipy\n",
    "from astropy.time import Time\n",
    "print(geopandas.__version__) # Version 0.8.1\n",
    "\n",
    "import rasterio\n",
    "import geopandas as gpd\n",
    "import pyproj\n",
    "import rasterio\n",
    "import glob\n",
    "from numpy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e76cb00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import glob\n",
    "from numpy import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187fbc96",
   "metadata": {},
   "source": [
    "getATL03 function loads in raw ICESat-2 ATL03 data in .h5 format and outputs are geodataframe where each row represents a photon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "750d3aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getATL03(f,beam):\n",
    "    # height of each received photon, relative to the WGS-84 ellipsoid (with some, not all corrections applied, see background info above)\n",
    "    heights=f[beam]['heights']['h_ph'][:]\n",
    "    # latitude (decimal degrees) of each received photon\n",
    "    lats=f[beam]['heights']['lat_ph'][:]\n",
    "    # longitude (decimal degrees) of each received photon\n",
    "    lons=f[beam]['heights']['lon_ph'][:]\n",
    "    # seconds from ATLAS Standard Data Product Epoch. use the epoch parameter to convert to gps time\n",
    "    dt=f[beam]['heights']['delta_time'][:]\n",
    "    # confidence level associated with each photon event\n",
    "    # -2: TEP\n",
    "    # -1: Events not associated with a specific surface type\n",
    "    #  0: noise\n",
    "    #  1: buffer but algorithm classifies as background\n",
    "    #  2: low\n",
    "    #  3: medium\n",
    "    #  4: high\n",
    "    # Surface types for signal classification confidence\n",
    "    # 0=Land; 1=Ocean; 2=SeaIce; 3=LandIce; 4=InlandWater    \n",
    "    conf=f[beam]['heights']['signal_conf_ph'][:,0] #choose column 2 for confidence of sea ice photons\n",
    "    # number of ATL03 20m segments\n",
    "    n_seg, = f[beam]['geolocation']['segment_id'].shape\n",
    "    #GEOID\n",
    "    geoid = f[beam]['geophys_corr']['geoid'][:]\n",
    "    # first photon in the segment (convert to 0-based indexing)\n",
    "    Segment_Index_begin = f[beam]['geolocation']['ph_index_beg'][:] - 1\n",
    "    # number of photon events in the segment\n",
    "    Segment_PE_count = f[beam]['geolocation']['segment_ph_cnt'][:]\n",
    "    # along-track distance for each ATL03 segment\n",
    "    Segment_Distance = f[beam]['geolocation']['segment_dist_x'][:]\n",
    "    # along-track distance (x) for photon events\n",
    "    x_atc = np.copy(f[beam]['heights']['dist_ph_along'][:])\n",
    "    # cross-track distance (y) for photon events\n",
    "    y_atc = np.copy(f[beam]['heights']['dist_ph_across'][:])\n",
    "\n",
    "    for j in range(n_seg):\n",
    "        # index for 20m segment j\n",
    "        idx = Segment_Index_begin[j]\n",
    "        # number of photons in 20m segment\n",
    "        cnt = Segment_PE_count[j]\n",
    "        # add segment distance to along-track coordinates\n",
    "        x_atc[idx:idx+cnt] += Segment_Distance[j]\n",
    "        #geoid\n",
    "        #geoid[idx:idx+cnt] += geoid[j]\n",
    "\n",
    "\n",
    "    df03=pd.DataFrame({'lats':lats,'lons':lons,'x':x_atc,'y':y_atc,'heights':heights,'dt':dt,'conf':conf})\n",
    "    return df03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "36411cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ATL03(path, date, beam):\n",
    "    ATL03_path = path + date + '/ATL03/'\n",
    "    for file in os.listdir(ATL03_path):\n",
    "        if file.endswith('.h5'):\n",
    "            fname = file\n",
    "    f = h5py.File(ATL03_path+fname,'r')\n",
    "    geoid = f[beam]['geophys_corr']['geoid'][:]\n",
    "    #10m res\n",
    "    geoid = list(np.repeat(geoid, 2))\n",
    "    #100m res\n",
    "    #geoid = geoid[::5]\n",
    "    #geoid = geoid.tolist()\n",
    "    df = getATL03(f,beam)\n",
    "    df = df[df['conf'] > 2]\n",
    "    df['AT_dist']=df.x-df.x.values[0]\n",
    "    return df, geoid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98570b3d",
   "metadata": {},
   "source": [
    "Surf function estimates the surface height by grouping photons height values along-track, using kernel density to estimate surface height, and substracting the geoid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "1c151b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Surf(dfATL03, window_width, geoid):\n",
    "    startTime = time.time()\n",
    "    \n",
    "    # Along Track Distance Values\n",
    "    AT_dist_values = dfATL03['AT_dist'].values\n",
    "    AT_dist_totmax = AT_dist_values.max()\n",
    "    AT_dist_totmin = AT_dist_values.min()\n",
    "    \n",
    "    #dfATL03 = dfATL03[dfATL03['conf'] > 2]\n",
    "    #Heights\n",
    "    heights_values = dfATL03['heights'].values\n",
    "    conf_values = dfATL03['conf'].values\n",
    "\n",
    "    #Coordinates\n",
    "    lats = dfATL03['lats'].values\n",
    "    lons = dfATL03['lons'].values\n",
    "    \n",
    "    #Moving Window\n",
    "    windows = np.arange(AT_dist_totmin, AT_dist_totmax, 10).tolist()\n",
    "\n",
    "    #print('df',dfATL03.shape)\n",
    "    #print(dfATL03)\n",
    "\n",
    "    #Empty array for AT_dist_Values\n",
    "    a = np.empty(len(windows))\n",
    "    b = np.empty(len(windows))  \n",
    "    c = np.empty(len(windows))  \n",
    "    d = np.empty(len(windows))  \n",
    "    \n",
    "    i = 0\n",
    "    #Iterate through rows in dataframe\n",
    "    for window_center in windows:    \n",
    "\n",
    "        #---------------\n",
    "        #Step 1: Window Boundries\n",
    "        #---------------\n",
    "\n",
    "        #Get minimum window boundries\n",
    "        min_dist = window_center - window_width\n",
    "        min_dist_array = np.where(AT_dist_values > min_dist)\n",
    "        min_dist_row = min_dist_array[0][0]\n",
    "\n",
    "        #Get maximum window boundries\n",
    "        max_dist = window_center + window_width\n",
    "        if max_dist < AT_dist_values[-1]:\n",
    "            max_dist_array = np.where(AT_dist_values > max_dist)\n",
    "            max_dist_row = max_dist_array[0][0]\n",
    "        else:\n",
    "            max_dist = AT_dist_values[-1] \n",
    "\n",
    "        #Get window center lat & long for plotting later        \n",
    "        idx = (np.abs(AT_dist_values - window_center)).argmin()\n",
    "        lat = lats[idx]\n",
    "        lon = lons[idx]\n",
    "\n",
    "        #Select photons AT_dist & heights within boundries\n",
    "        window_heights = heights_values[min_dist_row:max_dist_row]\n",
    "\n",
    "        #print(len(window_heights), 'win len')\n",
    "        #Only Search for surface if there is enough Photons in window\n",
    "        if len(window_heights) > 5: #Normally 5\n",
    "            \n",
    "        \n",
    "            binwidth = 0.2\n",
    "            #Kernel Density Function\n",
    "            kde = KernelDensity(kernel='gaussian', bandwidth=binwidth).fit(window_heights[:, None])\n",
    "            kde_scores = kde.score_samples(window_heights[:, None])\n",
    "            maxdensityI = np.max(kde_scores)\n",
    "            max_index = np.where(kde_scores == maxdensityI)\n",
    "            maxdensityH =  window_heights[max_index]\n",
    "            Surface_Estimation = maxdensityH[0]\n",
    "            a[i] = window_center #AT_DIST of window Center\n",
    "            b[i] = Surface_Estimation\n",
    "            c[i] = lat\n",
    "            d[i] = lon\n",
    "            \n",
    "            i+=1\n",
    "\n",
    "        else:\n",
    "            a[i] = window_center\n",
    "            b[i] = 0\n",
    "            c[i] = lat\n",
    "            d[i] = lon     \n",
    "            i+=1\n",
    "\n",
    "        #------------------\n",
    "        #Step 7: Outputs\n",
    "        #------------------\n",
    "\n",
    "        wincen = int(window_center)\n",
    "    \n",
    "        if wincen % 25000 < 1:  \n",
    "            m = int(i / len(windows) * 100)\n",
    "            print(m, \"% Done\")\n",
    "      \n",
    "    #pd.set_option('display.max_colwidth', None)\n",
    "    df = pd.DataFrame(a, columns = ['AT_dist']) \n",
    "    df[\"Surf_mean\"] = b\n",
    "    df[\"lats\"] = c\n",
    "    df[\"lons\"] = d\n",
    "\n",
    "    #Add geoid to correct surf height.\n",
    "    if len(geoid) < len(df.index):\n",
    "      while len(geoid) < len(df.index):\n",
    "        geoid.append(geoid[-1])\n",
    "      df[\"geoid\"] = geoid\n",
    "\n",
    "    elif len(geoid) > len(df.index):\n",
    "       df[\"geoid\"] = geoid[:len(df.index)]\n",
    "      \n",
    "    else:\n",
    "      df[\"geoid\"] = geoid\n",
    "\n",
    "    #Estimate surface height\n",
    "    df['Est_Surf'] = (df['Surf_mean'] - df['geoid']).round(2)\n",
    "    # Script Runtime\n",
    "    runTime =  int(time.time() - startTime)\n",
    "    runTimeMin = runTime/60\n",
    "    runTimeSec = runTime%60\n",
    "    print(\"\\nScript Runtime: %i minutes and %i seconds\" % (runTimeMin,runTimeSec))\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fba3bac",
   "metadata": {},
   "source": [
    "df_to_shp converts every estimated surface height to a point in a shapefile. These points are used further on for snow depth estimations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b871dab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def snow_depth(df, beam, DEM):\n",
    "    \n",
    "        filename = beam\n",
    "        pts = df\n",
    "        pts = geopandas.GeoDataFrame(\n",
    "        pts, geometry=geopandas.points_from_xy(df.lons, df.lats))\n",
    "            \n",
    "        pts = pts.set_crs('EPSG:4326')\n",
    "        pts = pts.to_crs(25833)       \n",
    "        \n",
    "        coords = [(x,y) for x, y in zip(pts.geometry.x, pts.geometry.y)]\n",
    "        src = rasterio.open(DEM)\n",
    "\n",
    "        pts['DEM'] = [x[0] for x in src.sample(coords)]\n",
    "        \n",
    "        #Norwegian DEM\n",
    "        pts['SD'] = round(pts.Est_Surf - pts.DEM,2)\n",
    "        \n",
    "        #Arctic DEM\n",
    "        #pts['SD'] = round(pts.Surf_mean - pts.DEM,2)\n",
    "        \n",
    "        print('Training Points: ',pts.shape[0])\n",
    "\n",
    "\n",
    "        return pts\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6743808e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Mask(df, path, date):\n",
    "    \n",
    "    pts = df\n",
    "    coords = [(x,y) for x, y in zip(pts.geometry.x, pts.geometry.y)]\n",
    "\n",
    "    SLOPE = path[:-7] + '/Masks/Slope.tif'\n",
    "    Binary_Mask = path + date + '/Masks/Binary_Mask.tif'\n",
    "    listofmasks=[SLOPE, Binary_Mask]\n",
    "    listofmasksname=['SLOPE', 'Binary_Mask']\n",
    "    \n",
    "    i = 0\n",
    "    for mask in listofmasks:\n",
    "        src = rasterio.open(mask)\n",
    "        pts[listofmasksname[i]] = [x[0] for x in src.sample(coords)]\n",
    "        i=i+1\n",
    "        \n",
    "    \n",
    "    #print(pts)\n",
    "    print('Training Points: ',pts.shape[0])\n",
    "\n",
    "    #Mask out points on slope over 8 degrees\n",
    "    pts = pts[pts['SLOPE'] < 8]\n",
    "    print('Slope: ',pts.shape[0])\n",
    "    \n",
    "    pts = pts[pts['Binary_Mask'] > 0]\n",
    "    print('Binary Mask: ',pts.shape[0])\n",
    "\n",
    "\n",
    "    #Mask out unrealistic snow depths\n",
    "    #pts = pts[pts['SD'] < 8]\n",
    "    #pts = pts[pts['SD'] > 0]\n",
    "    print('Outliers: ',pts.shape[0])\n",
    "\n",
    "    print('Training data points left after masking: ', pts.shape[0])\n",
    "    \n",
    "    return pts\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "8169e75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Mask_OLDVERSION(df, SLOPE, WATER, FC, WSC, SC):\n",
    "    \n",
    "    pts = df\n",
    "    coords = [(x,y) for x, y in zip(pts.geometry.x, pts.geometry.y)]\n",
    "    \n",
    "    listofmasks=[SLOPE, WATER, FC, WSC, SC]\n",
    "    listofmasksname=['SLOPE', 'WATER', 'FC', 'WSC', 'SC']\n",
    "    \n",
    "    i = 0\n",
    "    for mask in listofmasks:\n",
    "        src = rasterio.open(mask)\n",
    "        pts[listofmasksname[i]] = [x[0] for x in src.sample(coords)]\n",
    "        i=i+1\n",
    "        \n",
    "    \n",
    "    #print(pts)\n",
    "    print('Training Points: ',pts.shape[0])\n",
    "\n",
    "    #Mask out points on slope over 8 degrees\n",
    "    pts = pts[pts['SLOPE'] < 8]\n",
    "    print('Slope: ',pts.shape[0])\n",
    "\n",
    "    #Mask out points on water\n",
    "    pts = pts[pts['WATER'] < 1]\n",
    "    print('water: ',pts.shape[0])\n",
    "\n",
    "    #Mask out points located in the forest\n",
    "    pts = pts[pts['FC'] > 0]\n",
    "    print('fc: ',pts.shape[0])\n",
    "\n",
    "    #Mask out points located on wet snow (VH < -21)\n",
    "    pts = pts[pts['WSC'] > 0]\n",
    "    print('wsc: ',pts.shape[0])\n",
    "\n",
    "    #Mask out unrealistic snow depths\n",
    "    pts = pts[pts['SD'] < 10]\n",
    "    pts = pts[pts['SD'] > -1]\n",
    "    print('Outliers: ',pts.shape[0])\n",
    "\n",
    "    print('Training data points left after masking: ', pts.shape[0])\n",
    "    \n",
    "    return pts\n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2c501c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Rescale(df, window_width):\n",
    "\n",
    "        pts = df\n",
    "        \n",
    "\n",
    "            # Along Track Distance Values\n",
    "        AT_dist_values = pts['AT_dist'].values\n",
    "        AT_dist_totmax = AT_dist_values.max()\n",
    "        AT_dist_totmin = AT_dist_values.min()\n",
    "\n",
    "                    #Coordinates\n",
    "        pts['lons'] = pts.geometry.x\n",
    "        pts['lats']= pts.geometry.y\n",
    "        \n",
    "        \n",
    "        lats = pts['lats'].values\n",
    "        lons = pts['lons'].values\n",
    "\n",
    "        SD_values = pts['SD'].values\n",
    "        \n",
    "\n",
    "            #Moving Window\n",
    "        windows = np.arange(AT_dist_totmin, AT_dist_totmax, 1000).tolist()\n",
    "\n",
    "        a = np.empty(len(windows))\n",
    "        b = np.empty(len(windows))  \n",
    "        c = np.empty(len(windows))  \n",
    "        d = np.empty(len(windows))  \n",
    "\n",
    "        i = 0\n",
    "\n",
    "        for window_center in windows:\n",
    "\n",
    "            #Get minimum window boundries\n",
    "            min_dist = window_center - window_width\n",
    "            min_dist_array = np.where(AT_dist_values > min_dist)\n",
    "            min_dist_row = min_dist_array[0][0]\n",
    "\n",
    "            #Get maximum window boundries\n",
    "            max_dist = window_center + window_width\n",
    "            if max_dist < AT_dist_values[-1]:\n",
    "                max_dist_array = np.where(AT_dist_values > max_dist)\n",
    "                max_dist_row = max_dist_array[0][0]\n",
    "            else:\n",
    "                max_dist = AT_dist_values[-1] \n",
    "\n",
    "            #Get window center lat & long for plotting later        \n",
    "            idx = (np.abs(AT_dist_values - window_center)).argmin()\n",
    "            lat = lats[idx]\n",
    "            lon = lons[idx]\n",
    "                #print('df',dfATL03.shape)\n",
    "\n",
    "            window_SD = SD_values[min_dist_row:max_dist_row]\n",
    "            #print('len',len(window_SD))\n",
    "            if len(window_SD) > (5): #Normally 5\n",
    "                SD = np.median(window_SD)\n",
    "\n",
    "\n",
    "                a[i] = window_center #AT_DIST of window Center\n",
    "                b[i] = SD\n",
    "                c[i] = lat\n",
    "                d[i] = lon\n",
    "\n",
    "                i+=1\n",
    "\n",
    "            else:\n",
    "                a[i] = window_center\n",
    "                b[i] = -999\n",
    "                c[i] = lat\n",
    "                d[i] = lon\n",
    "                i+=1\n",
    "\n",
    "        df = gpd.GeoDataFrame(a, columns = ['AT_dist']) \n",
    "        df[\"SD\"] = b\n",
    "        df[\"lats\"] = c\n",
    "        df[\"lons\"] = d\n",
    "\n",
    "        gdf = geopandas.GeoDataFrame(\n",
    "        df, geometry=geopandas.points_from_xy(df.lons, df.lats))\n",
    "        gdf = gdf.set_crs('EPSG:25833')\n",
    "        gdf = gdf[gdf['SD'] > 0]\n",
    "        gdf = gdf[gdf['SD'] < 10]\n",
    "        gdf = gdf[gdf['SD'] != -999]\n",
    "        #gdf.to_file(outdir + filename + '.shp')\n",
    "        return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "3ce941e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_shp(df,beam, path, date):\n",
    "    gdf = df\n",
    "    if gdf.shape[0] > 0:\n",
    "    #gdf = geopandas.GeoDataFrame(\n",
    "    #df, geometry=geopandas.points_from_xy(df.lons, df.lats))\n",
    "    #print('crs',gdf.crs)\n",
    "    #gdf = gdf.set_crs('EPSG:4326')\n",
    "    #print('crs',gdf.crs)\n",
    "    #gdf = gdf.to_crs(25833)\n",
    "    #print('crs',gdf.crs)\n",
    "    #gdf.crs = {\"init\":\"epsg:25833\"}\n",
    "    #print('crs',gdf.crs)\n",
    "        outdir = path + date + '/SnowDepth/' + beam + '.shp'\n",
    "        gdf.to_file(outdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b7bd46d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sample(path, date):\n",
    "    \n",
    "    i=0\n",
    "    SD_path = path + date + '/SnowDepth/'\n",
    "    for file in os.listdir(SD_path):\n",
    "        if file.endswith('shp'):\n",
    "\n",
    "#            filename = file[3:7]\n",
    "            points = os.path.join(SD_path,file)\n",
    "            pts = gpd.read_file(points)\n",
    "            \n",
    "            if i != 0:\n",
    "                df_train = pd.concat([df_train,pts])\n",
    "            else: \n",
    "                df_train = pts\n",
    "                i=i+1  \n",
    "                \n",
    "    pts = df_train\n",
    "    coords = [(x,y) for x, y in zip(pts.geometry.x, pts.geometry.y)]\n",
    "    \n",
    "    #VH Diff: Substract of VH from fall baseline median to target date VH. (fx march)\n",
    "    #Ratio: VH divided with VV (or other way around, forgot)\n",
    "    #Subtract: VH subtract with VV (or other way around, forgot)\n",
    "    VH = path + date + '/S1/VH.tif'\n",
    "    Diff = path + date + '/S1/Diff.tif'\n",
    "    Ratio = path + date + '/S1/Ratio.tif'\n",
    "    Subtract = path + date + '/S1/Subtract.tif'\n",
    "    DEM = path[:-7] + '/DEM/ArcticDEM_10m.tif'\n",
    "    \n",
    "    outdir = path + date + '/Sampled/SD.shp'\n",
    "    \n",
    "    listofmasks=[VH, Diff, Ratio, Subtract, DEM]\n",
    "    listofmasksname=['VH', 'Diff', 'Ratio', 'Subtract','DEM']\n",
    "    i = 0\n",
    "    for mask in listofmasks:\n",
    "        src = rasterio.open(mask)\n",
    "        pts[listofmasksname[i]] = [x[0] for x in src.sample(coords)]\n",
    "        i=i+1\n",
    "    pts = pts.drop(columns=['lats', 'lons', 'AT_dist'])    \n",
    "    pts.to_file(outdir)\n",
    "    return pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "1c6f5ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(params):\n",
    "\n",
    "    #datelist = ['20200204','20200206','20200306','20200314','20200406','20200410','20210203','20210205','20210207','20210209','20210211','20210304','20210308']\n",
    "    datelist = ['20210211','20210304','20210308']\n",
    "    for dates in datelist:\n",
    "        params['date'] = dates\n",
    "        print('started processing of date: ', dates)\n",
    "    \n",
    "    \n",
    "        beamlist = ['gt1l','gt2l','gt3l','gt1r','gt2r','gt3r']\n",
    "        #beamlist = ['gt1l','gt2l']\n",
    "\n",
    "    #    i=0\n",
    "        for ISbeam in beamlist:\n",
    "            params['beam'] = ISbeam\n",
    "            df, geoid = load_ATL03(params['path'],params['date'], params['beam'])\n",
    "            print('ATL03 Dataset loaded')\n",
    "\n",
    "            df_surf = Surf(df, 10, geoid)\n",
    "            df_surf = df_surf[df_surf['Surf_mean'] != 0]\n",
    "            print('Surface Heights Estimated')\n",
    "\n",
    "            df_SD = snow_depth(df_surf, params['beam'], params['DEM'])\n",
    "            print('Snow Depths Estimated')\n",
    "\n",
    "            df_Mask = Mask(df_SD, params['path'], params['date'])\n",
    "            print('Masked out training points')\n",
    "\n",
    "            df_RE = Rescale(df_Mask, 3000)\n",
    "            print('Rescaled training points')\n",
    "\n",
    "            shp = df_to_shp(df_RE,params['beam'], params['path'], params['date'])\n",
    "            print('completed ICESat-2 Processing for beam: ',ISbeam)\n",
    "\n",
    "            pts = Sample(params['path'], params['date'])\n",
    "        \n",
    "    return df, df_surf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "bdba9340",
   "metadata": {},
   "outputs": [],
   "source": [
    "Load_params = {\n",
    "    'path' : 'C:/Users/Rasmu/Documents/Thesis/Hardangervidda/Dates/',\n",
    "    'date' : '20200306',\n",
    "    #'DEM': 'C:/Users/Rasmu/Documents/Thesis/DEM/ArcticDEM_10m.tif',\n",
    "    'DEM': 'C:/Users/Rasmu/Documents/Thesis/DEM/mergedDEM10m_Hardanger.tif'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "f5593dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def Multitemporal_run(path):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "67d1a5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started processing of date:  20210211\n",
      "ATL03 Dataset loaded\n",
      "0 % Done\n",
      "25 % Done\n",
      "51 % Done\n",
      "76 % Done\n",
      "\n",
      "Script Runtime: 0 minutes and 3 seconds\n",
      "Surface Heights Estimated\n",
      "Training Points:  3080\n",
      "Snow Depths Estimated\n",
      "Training Points:  3080\n",
      "Slope:  1787\n",
      "Binary Mask:  567\n",
      "Outliers:  567\n",
      "Training data points left after masking:  567\n",
      "Masked out training points\n",
      "Rescaled training points\n",
      "completed ICESat-2 Processing for beam:  gt1l\n",
      "ATL03 Dataset loaded\n",
      "0 % Done\n",
      "25 % Done\n",
      "50 % Done\n",
      "76 % Done\n",
      "\n",
      "Script Runtime: 0 minutes and 6 seconds\n",
      "Surface Heights Estimated\n",
      "Training Points:  4801\n",
      "Snow Depths Estimated\n",
      "Training Points:  4801\n",
      "Slope:  3187\n",
      "Binary Mask:  1590\n",
      "Outliers:  1590\n",
      "Training data points left after masking:  1590\n",
      "Masked out training points\n",
      "Rescaled training points\n",
      "completed ICESat-2 Processing for beam:  gt2l\n",
      "ATL03 Dataset loaded\n",
      "0 % Done\n",
      "25 % Done\n",
      "50 % Done\n",
      "76 % Done\n",
      "\n",
      "Script Runtime: 0 minutes and 5 seconds\n",
      "Surface Heights Estimated\n",
      "Training Points:  3975\n",
      "Snow Depths Estimated\n",
      "Training Points:  3975\n",
      "Slope:  2818\n",
      "Binary Mask:  1117\n",
      "Outliers:  1117\n",
      "Training data points left after masking:  1117\n",
      "Masked out training points\n",
      "Rescaled training points\n",
      "completed ICESat-2 Processing for beam:  gt3l\n",
      "ATL03 Dataset loaded\n",
      "0 % Done\n",
      "25 % Done\n",
      "50 % Done\n",
      "75 % Done\n",
      "\n",
      "Script Runtime: 1 minutes and 39 seconds\n",
      "Surface Heights Estimated\n",
      "Training Points:  9282\n",
      "Snow Depths Estimated\n",
      "Training Points:  9282\n",
      "Slope:  3116\n",
      "Binary Mask:  1097\n",
      "Outliers:  1097\n",
      "Training data points left after masking:  1097\n",
      "Masked out training points\n",
      "Rescaled training points\n",
      "completed ICESat-2 Processing for beam:  gt1r\n",
      "ATL03 Dataset loaded\n",
      "0 % Done\n",
      "25 % Done\n",
      "50 % Done\n",
      "75 % Done\n",
      "\n",
      "Script Runtime: 1 minutes and 7 seconds\n",
      "Surface Heights Estimated\n",
      "Training Points:  9212\n",
      "Snow Depths Estimated\n",
      "Training Points:  9212\n",
      "Slope:  4264\n",
      "Binary Mask:  2062\n",
      "Outliers:  2062\n",
      "Training data points left after masking:  2062\n",
      "Masked out training points\n",
      "Rescaled training points\n",
      "completed ICESat-2 Processing for beam:  gt2r\n",
      "ATL03 Dataset loaded\n",
      "0 % Done\n",
      "25 % Done\n",
      "50 % Done\n",
      "75 % Done\n",
      "\n",
      "Script Runtime: 1 minutes and 14 seconds\n",
      "Surface Heights Estimated\n",
      "Training Points:  9332\n",
      "Snow Depths Estimated\n",
      "Training Points:  9332\n",
      "Slope:  4020\n",
      "Binary Mask:  1326\n",
      "Outliers:  1326\n",
      "Training data points left after masking:  1326\n",
      "Masked out training points\n",
      "Rescaled training points\n",
      "completed ICESat-2 Processing for beam:  gt3r\n",
      "started processing of date:  20210304\n",
      "ATL03 Dataset loaded\n",
      "0 % Done\n",
      "29 % Done\n",
      "59 % Done\n",
      "89 % Done\n",
      "\n",
      "Script Runtime: 0 minutes and 3 seconds\n",
      "Surface Heights Estimated\n",
      "Training Points:  3133\n",
      "Snow Depths Estimated\n",
      "Training Points:  3133\n",
      "Slope:  2788\n",
      "Binary Mask:  1045\n",
      "Outliers:  1045\n",
      "Training data points left after masking:  1045\n",
      "Masked out training points\n",
      "Rescaled training points\n",
      "completed ICESat-2 Processing for beam:  gt1l\n",
      "ATL03 Dataset loaded\n",
      "0 % Done\n",
      "29 % Done\n",
      "58 % Done\n",
      "88 % Done\n",
      "\n",
      "Script Runtime: 0 minutes and 3 seconds\n",
      "Surface Heights Estimated\n",
      "Training Points:  3917\n",
      "Snow Depths Estimated\n",
      "Training Points:  3917\n",
      "Slope:  3432\n",
      "Binary Mask:  1300\n",
      "Outliers:  1300\n",
      "Training data points left after masking:  1300\n",
      "Masked out training points\n",
      "Rescaled training points\n",
      "completed ICESat-2 Processing for beam:  gt2l\n",
      "ATL03 Dataset loaded\n",
      "0 % Done\n",
      "29 % Done\n",
      "58 % Done\n",
      "88 % Done\n",
      "\n",
      "Script Runtime: 0 minutes and 3 seconds\n",
      "Surface Heights Estimated\n",
      "Training Points:  3325\n",
      "Snow Depths Estimated\n",
      "Training Points:  3325\n",
      "Slope:  2935\n",
      "Binary Mask:  1119\n",
      "Outliers:  1119\n",
      "Training data points left after masking:  1119\n",
      "Masked out training points\n",
      "Rescaled training points\n",
      "completed ICESat-2 Processing for beam:  gt3l\n",
      "ATL03 Dataset loaded\n",
      "0 % Done\n",
      "29 % Done\n",
      "58 % Done\n",
      "87 % Done\n",
      "\n",
      "Script Runtime: 1 minutes and 5 seconds\n",
      "Surface Heights Estimated\n",
      "Training Points:  8318\n",
      "Snow Depths Estimated\n",
      "Training Points:  8318\n",
      "Slope:  5019\n",
      "Binary Mask:  2197\n",
      "Outliers:  2197\n",
      "Training data points left after masking:  2197\n",
      "Masked out training points\n",
      "Rescaled training points\n",
      "completed ICESat-2 Processing for beam:  gt1r\n",
      "ATL03 Dataset loaded\n",
      "0 % Done\n",
      "29 % Done\n",
      "58 % Done\n",
      "88 % Done\n",
      "\n",
      "Script Runtime: 0 minutes and 51 seconds\n",
      "Surface Heights Estimated\n",
      "Training Points:  8278\n",
      "Snow Depths Estimated\n",
      "Training Points:  8278\n",
      "Slope:  5552\n",
      "Binary Mask:  2509\n",
      "Outliers:  2509\n",
      "Training data points left after masking:  2509\n",
      "Masked out training points\n",
      "Rescaled training points\n",
      "completed ICESat-2 Processing for beam:  gt2r\n",
      "ATL03 Dataset loaded\n",
      "0 % Done\n",
      "29 % Done\n",
      "58 % Done\n",
      "88 % Done\n",
      "\n",
      "Script Runtime: 1 minutes and 8 seconds\n",
      "Surface Heights Estimated\n",
      "Training Points:  8303\n",
      "Snow Depths Estimated\n",
      "Training Points:  8303\n",
      "Slope:  5132\n",
      "Binary Mask:  2470\n",
      "Outliers:  2470\n",
      "Training data points left after masking:  2470\n",
      "Masked out training points\n",
      "Rescaled training points\n",
      "completed ICESat-2 Processing for beam:  gt3r\n",
      "started processing of date:  20210308\n",
      "ATL03 Dataset loaded\n",
      "0 % Done\n",
      "20 % Done\n",
      "41 % Done\n",
      "62 % Done\n",
      "83 % Done\n",
      "\n",
      "Script Runtime: 0 minutes and 3 seconds\n",
      "Surface Heights Estimated\n",
      "Training Points:  2694\n",
      "Snow Depths Estimated\n",
      "Training Points:  2694\n",
      "Slope:  2562\n",
      "Binary Mask:  1281\n",
      "Outliers:  1281\n",
      "Training data points left after masking:  1281\n",
      "Masked out training points\n",
      "Rescaled training points\n",
      "completed ICESat-2 Processing for beam:  gt1l\n",
      "ATL03 Dataset loaded\n",
      "0 % Done\n",
      "20 % Done\n",
      "41 % Done\n",
      "61 % Done\n",
      "82 % Done\n",
      "\n",
      "Script Runtime: 0 minutes and 3 seconds\n",
      "Surface Heights Estimated\n",
      "Training Points:  2223\n",
      "Snow Depths Estimated\n",
      "Training Points:  2223\n",
      "Slope:  2020\n",
      "Binary Mask:  925\n",
      "Outliers:  925\n",
      "Training data points left after masking:  925\n",
      "Masked out training points\n",
      "Rescaled training points\n",
      "completed ICESat-2 Processing for beam:  gt2l\n",
      "ATL03 Dataset loaded\n",
      "0 % Done\n",
      "20 % Done\n",
      "41 % Done\n",
      "62 % Done\n",
      "83 % Done\n",
      "\n",
      "Script Runtime: 0 minutes and 2 seconds\n",
      "Surface Heights Estimated\n",
      "Training Points:  1773\n",
      "Snow Depths Estimated\n",
      "Training Points:  1773\n",
      "Slope:  1684\n",
      "Binary Mask:  655\n",
      "Outliers:  655\n",
      "Training data points left after masking:  655\n",
      "Masked out training points\n",
      "Rescaled training points\n",
      "completed ICESat-2 Processing for beam:  gt3l\n",
      "ATL03 Dataset loaded\n",
      "0 % Done\n",
      "20 % Done\n",
      "41 % Done\n",
      "61 % Done\n",
      "82 % Done\n",
      "\n",
      "Script Runtime: 1 minutes and 29 seconds\n",
      "Surface Heights Estimated\n",
      "Training Points:  10479\n",
      "Snow Depths Estimated\n",
      "Training Points:  10479\n",
      "Slope:  7075\n",
      "Binary Mask:  3694\n",
      "Outliers:  3694\n",
      "Training data points left after masking:  3694\n",
      "Masked out training points\n",
      "Rescaled training points\n",
      "completed ICESat-2 Processing for beam:  gt1r\n",
      "ATL03 Dataset loaded\n",
      "0 % Done\n",
      "20 % Done\n",
      "41 % Done\n",
      "61 % Done\n",
      "82 % Done\n",
      "\n",
      "Script Runtime: 1 minutes and 2 seconds\n",
      "Surface Heights Estimated\n",
      "Training Points:  10163\n",
      "Snow Depths Estimated\n",
      "Training Points:  10163\n",
      "Slope:  6118\n",
      "Binary Mask:  3339\n",
      "Outliers:  3339\n",
      "Training data points left after masking:  3339\n",
      "Masked out training points\n",
      "Rescaled training points\n",
      "completed ICESat-2 Processing for beam:  gt2r\n",
      "ATL03 Dataset loaded\n",
      "0 % Done\n",
      "20 % Done\n",
      "41 % Done\n",
      "61 % Done\n",
      "82 % Done\n",
      "\n",
      "Script Runtime: 1 minutes and 23 seconds\n",
      "Surface Heights Estimated\n",
      "Training Points:  10502\n",
      "Snow Depths Estimated\n",
      "Training Points:  10502\n",
      "Slope:  6896\n",
      "Binary Mask:  3837\n",
      "Outliers:  3837\n",
      "Training data points left after masking:  3837\n",
      "Masked out training points\n",
      "Rescaled training points\n",
      "completed ICESat-2 Processing for beam:  gt3r\n"
     ]
    }
   ],
   "source": [
    "df, df_surf = run(Load_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b2c487",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
